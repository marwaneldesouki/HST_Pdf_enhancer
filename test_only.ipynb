{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d2af9df",
   "metadata": {},
   "source": [
    "# it`s not official just use it as a test#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0441decc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import camelot\n",
    "from dotenv import load_dotenv\n",
    "import arabic_reshaper\n",
    "from bidi.algorithm import get_display\n",
    "from joblib import Memory\n",
    "from pdf2image import convert_from_path\n",
    "from PIL import Image\n",
    "import pytesseract\n",
    "from pytesseract import Output\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d2645fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define a directory for cache storage\n",
    "memory = Memory(\"./cache_dir\", verbose=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3a8d793",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDF files found:\n",
      "{'path': '.\\\\documents\\\\الاثنين 16-12-2024 د19 القاهرة الجديدة.pdf'}\n",
      "{'path': '.\\\\documents\\\\الاثنين جلسة 1-1-2024 د19.pdf'}\n",
      "{'path': '.\\\\documents\\\\الاثنين جلسة 1-4-2024 د19 وراثات.pdf'}\n",
      "{'path': '.\\\\documents\\\\د(19)اسرة السيدة زينب 22-12-2024.pdf'}\n",
      "{'path': '.\\\\documents\\\\د(20)اسرة باب الشعرية 19-12-2024.pdf'}\n",
      "{'path': '.\\\\documents\\\\د(20)اسرة عابدين18-12-2024.pdf'}\n",
      "{'path': '.\\\\documents\\\\د(22) اسرة  الموسكى  جلسة 21-12-2024.pdf'}\n",
      "{'path': '.\\\\documents\\\\وراثات الزاوية 16-12.pdf'}\n",
      "{'path': '.\\\\documents\\\\وراثات الشرابية 17-12.pdf'}\n"
     ]
    }
   ],
   "source": [
    "directory_path = r\".\\documents\"\n",
    "\n",
    "pdf_files = []\n",
    "for root, dirs, files in os.walk(directory_path):\n",
    "    for file in files:\n",
    "        if file.endswith('.pdf'):\n",
    "            pdf_files.append({\"path\":os.path.join(root, file)})\n",
    "\n",
    "# Print the list of PDF files with their paths\n",
    "print(\"PDF files found:\")\n",
    "for pdf_file in pdf_files:\n",
    "    print(pdf_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "62f54d3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'path': '.\\\\documents\\\\الاثنين 16-12-2024 د19 القاهرة الجديدة.pdf'}, {'path': '.\\\\documents\\\\الاثنين جلسة 1-1-2024 د19.pdf'}, {'path': '.\\\\documents\\\\الاثنين جلسة 1-4-2024 د19 وراثات.pdf'}, {'path': '.\\\\documents\\\\د(19)اسرة السيدة زينب 22-12-2024.pdf'}, {'path': '.\\\\documents\\\\د(20)اسرة باب الشعرية 19-12-2024.pdf'}, {'path': '.\\\\documents\\\\د(20)اسرة عابدين18-12-2024.pdf'}, {'path': '.\\\\documents\\\\د(22) اسرة  الموسكى  جلسة 21-12-2024.pdf'}, {'path': '.\\\\documents\\\\وراثات الزاوية 16-12.pdf'}, {'path': '.\\\\documents\\\\وراثات الشرابية 17-12.pdf'}]\n"
     ]
    }
   ],
   "source": [
    "@memory.cache\n",
    "def get_pdf_tables(pdf_docs):\n",
    "    \"\"\"\n",
    "    this function reads the pdf files and extracts the tables from the first page of each pdf file and crop the first page as an image\n",
    "    \"\"\"\n",
    "    for pdf in pdf_docs:\n",
    "        print(pdf['path'])\n",
    "        pdf_reader = camelot.read_pdf(pdf['path'],pages='all',) #address of pdf file(pdf)\n",
    "        pdf.update({'tablelist':pdf_reader})\n",
    "        pdf.update({'first_page_as_image':convert_from_path(pdf['path'], first_page=1, last_page=1, dpi=300,poppler_path=r\".\\libs\\poppler-24.08.0\\Library\\bin\")[0]})\n",
    "        \n",
    "get_pdf_tables(pdf_files)\n",
    "print(pdf_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5469e7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_arabic_text(cell): # Fix Arabic text direction\n",
    "    try:\n",
    "        reshaped_text = arabic_reshaper.reshape(cell)  # Reshape Arabic characters\n",
    "        bidi_text = get_display(reshaped_text)  # Apply BiDi algorithm\n",
    "        return bidi_text\n",
    "    except Exception:\n",
    "        return cell  # Return as is if not Arabic\n",
    "    \n",
    "@memory.cache\n",
    "def get_head_from_first_page(pdf): # Get the head of the table from the first page only of each PDF\n",
    "        first_page = pdf[0]\n",
    "        first_page.df = first_page.df.map(fix_arabic_text)\n",
    "        # Access the table's structure\n",
    "        cells = first_page.cells  # List of all detected cells\n",
    "        head = [int(cells[0][0].x1), int(cells[0][0].y2), int(cells[0][-1].x2), int(cells[0][0].y1)]\n",
    "        return head\n",
    "\n",
    "for pdf in pdf_files:\n",
    "    pdf.update({'head_coordinates':get_head_from_first_page(pdf['tablelist'])})\n",
    "\n",
    "print(pdf_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c151d665",
   "metadata": {},
   "source": [
    "## You don`t need to run this cell\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe9f3f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false\n",
    "def check_on_pdf_grid(pdf_path,key):\n",
    "    file_name =os.path.basename(pdf_path)\n",
    "    nested_tables = camelot.read_pdf(pdf_path, pages='1',flavor='stream',table_areas=[head_coordinates[key]],edge_tol=500,)\n",
    "    camelot.plot(table=nested_tables[0], kind=\"grid\")\n",
    "    \n",
    "for pdf_path,key in zip(pdf_files,head_coordinates):\n",
    "    print(pdf_path,key)\n",
    "    check_on_pdf_grid(pdf_path,key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1014037f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for pdf in pdf_files:\n",
    "    pdf_name = os.path.basename(pdf['path']).split('.')[0]\n",
    "    print(pdf_name)\n",
    "    if pdf_name.__contains__('وراثات'):\n",
    "        cropped_cell_image = pdf['first_page_as_image'].crop((pdf['head_coordinates'][0], 0, pdf['first_page_as_image'].width,(pdf['first_page_as_image'].height/100)+pdf['head_coordinates'][1]-205)) #x1,y2,x2,y1\n",
    "    else:\n",
    "        cropped_cell_image = pdf['first_page_as_image'].crop((pdf['head_coordinates'][0], 0, pdf['first_page_as_image'].width,(pdf['first_page_as_image'].height/100)+pdf['head_coordinates'][1])) #x1,y2,x2,y1\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    plt.imshow(np.array(cropped_cell_image))\n",
    "    plt.show()\n",
    "    cropped_cell_image = np.asarray(cropped_cell_image)\n",
    "    gray_image = cv2.cvtColor(cropped_cell_image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    min_val, max_val, _, _ = cv2.minMaxLoc(gray_image)\n",
    "\n",
    "    print(f\"Minimum Intensity: {min_val}\")\n",
    "    print(f\"Maximum Intensity: {max_val}\")\n",
    "\n",
    "    # Get the minimum and maximum intensity values\n",
    "    min_val = np.min(gray_image)\n",
    "    max_val = np.max(gray_image)\n",
    "\n",
    "    print(f\"Minimum Intensity: {min_val}\")\n",
    "    print(f\"Maximum Intensity: {max_val}\")\n",
    "\n",
    "\n",
    "    # Define a threshold value dynamically (e.g., mid-point)\n",
    "    threshold = (min_val + max_val) / 2\n",
    "\n",
    "    # Apply thresholding\n",
    "    _, binary_image = cv2.threshold(gray_image, threshold, 255, cv2.THRESH_BINARY)\n",
    "    pdf.update({\"eci\":binary_image}) #enhanced_cropped_images\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f0018008",
   "metadata": {},
   "outputs": [],
   "source": [
    "@memory.cache\n",
    "def extract_text_with_psm(image, lang='ara'):\n",
    "    \"\"\"\n",
    "    Run Tesseract with different PSM values and return the best result.\n",
    "    \"\"\"\n",
    "    # List of PSM modes to test\n",
    "    psm_modes = [3, 4, 6, 7, 11,12, 13]\n",
    "    best_text = \"\"\n",
    "    best_psm = None\n",
    "    highest_confidence = 0\n",
    "    \n",
    "    # Load the image\n",
    "    \n",
    "    for psm in psm_modes:\n",
    "        # Configure Tesseract with the current PSM\n",
    "        config = f\"--psm {psm} -l {lang}\"\n",
    "        data = pytesseract.image_to_data(image, config=config, output_type=Output.DICT)\n",
    "        \n",
    "        # Extract confidence scores and calculate average confidence\n",
    "        conf = [conf for conf in data['conf'] if isinstance(conf, int) or conf.isdigit()]\n",
    "        conf = list(map(int, conf))  # Ensure all confidence values are integers\n",
    "        avg_confidence = sum(conf) / len(conf) if conf else 0\n",
    "        \n",
    "        # Get the extracted text\n",
    "        text = pytesseract.image_to_string(image, config=config)\n",
    "        \n",
    "        # Update the best result if this PSM is better\n",
    "        if avg_confidence > highest_confidence:\n",
    "            highest_confidence = avg_confidence\n",
    "            best_text = text\n",
    "            best_psm = psm\n",
    "    \n",
    "    return best_psm, best_text, highest_confidence\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf2f56d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "def process_pdf(pdf):\n",
    "    best_psm, best_text, confidence = extract_text_with_psm(pdf['eci'])\n",
    "\n",
    "    print(f\"Best PSM: {best_psm}\")\n",
    "    print(f\"Confidence: {confidence}\")\n",
    "    print(\"Extracted Text:\")\n",
    "    print(best_text, \"\\n###############################################\\n\")\n",
    "    \n",
    "    pdf.update({\"BT\": best_text})\n",
    "\n",
    "    # Save the cropped image (optional, for debugging purposes)\n",
    "    image_path = fr\".\\cropped_images\\{os.path.basename(pdf['path']).split('.')[0]}.png\"\n",
    "    cv2.imwrite(image_path, pdf['eci'])\n",
    "    \n",
    "    # Write the extracted text to a file\n",
    "    text_file_path = fr\".\\cropped_images\\{os.path.basename(pdf['path']).split('.')[0]}.txt\"\n",
    "    with open(text_file_path, \"w\") as file:\n",
    "        file.write(best_text)\n",
    "\n",
    "def process_pdfs_in_parallel(pdf_files):\n",
    "    # Create a ThreadPoolExecutor to run the tasks concurrently\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        # Submit the function for each PDF in the pdf_files list\n",
    "        executor.map(process_pdf, pdf_files)\n",
    "        \n",
    "process_pdfs_in_parallel(pdf_files)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20bd320",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from difflib import get_close_matches\n",
    "\n",
    "\n",
    "# Function to clean punctuation or symbols from a word\n",
    "def clean_word(word):\n",
    "    # Use regular expression to remove unwanted punctuation from the edges\n",
    "    # cleaned_word = re.sub(r'^[^a-zA-Z0-9\\u0600-\\u06FF]+|[^a-zA-Z0-9\\u0600-\\u06FF]+$', '', word)\n",
    "     # Remove commas and dots explicitly\n",
    "    cleaned_word = word.strip(',').strip('.').strip('،').strip('\"').strip(\" َ\").strip('(').strip(')').replace('\"',\"\").strip()\n",
    "    return cleaned_word\n",
    "\n",
    "# Find closest matches for a word\n",
    "def correct_word(word, correct_words):\n",
    "    cleaned = clean_word(word)\n",
    "    matches = get_close_matches(cleaned, correct_words, n=1, cutoff=0.7)\n",
    "    return matches[0] if matches else cleaned\n",
    "\n",
    "def get_corrected_text(text:list[str],correct_words):\n",
    "    # Correct the text line by line\n",
    "    corrected_lines = []\n",
    "    break_on_line = 0\n",
    "    test_lines_nospaces = [x.strip() for x in text]\n",
    "    for line in test_lines_nospaces:\n",
    "        words = line.split()  # Split the line into words\n",
    "        for word in words:\n",
    "            cleaned = clean_word(word) # Clean the word\n",
    "            corrected = correct_word(cleaned,correct_words) # Find a correction\n",
    "            line = line.replace(word,corrected)\n",
    "            # print(f\"word:{word},cleaned:{cleaned},corrected:{corrected}\")\n",
    "        # if break_on_line == 5:\n",
    "        #         break\n",
    "        # break_on_line += 1\n",
    "        # Rebuild the line\n",
    "        corrected_lines.append(line)\n",
    "\n",
    "    # Join all corrected lines to recreate the original structure\n",
    "    corrected_text = '\\n'.join(corrected_lines)\n",
    "\n",
    "    return corrected_text\n",
    "\n",
    "\n",
    "for pdf in pdf_files:\n",
    "    # correct_lines = [line.strip() for line in pdf['tablelist'][0].df[0][0].readlines()]\n",
    "    correct_words = list([line.split() for line in pdf['tablelist'][0].df[0][0].splitlines()])\n",
    "    correct_words = [list(map(clean_word,element)) for element in correct_words]\n",
    "    correct_words = [item for sublist in correct_words for item in sublist]\n",
    "    correct_words = list(map(fix_arabic_text,correct_words))\n",
    "    best_text_splitted = [fix_arabic_text(line) for line in pdf['BT'].splitlines()[1:] if line]\n",
    "    corrected_text = get_corrected_text(best_text_splitted,correct_words)\n",
    "    corrected_text= fix_arabic_text(corrected_text)\n",
    "    print(corrected_text)\n",
    "    # break\n",
    "    # print(corrected_text)\n",
    "    print(\"###############################################\")\n",
    "    with open(fr\"D:\\my work\\python\\HST_Pdf_enhancer\\cropped_images\\{os.path.basename(pdf['path']).split('.')[0]}_corrected.txt\", \"w\", encoding=\"utf-8\") as corrected_file:\n",
    "        corrected_file.write(corrected_text)\n",
    "\n",
    "print(\"Correction completed and saved!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd81277",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_lines = [line.strip() for line in open(\"test_correct.txt\", encoding=\"utf-8\").readlines()]\n",
    "print(correct_lines)\n",
    "print(get_close_matches(\"باب\", correct_lines, n=3, cutoff=0.7))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
